%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FRI Data Science_report LaTeX Template
% Version 1.0 (28/1/2020)
% 
% Jure Demšar (jure.demsar@fri.uni-lj.si)
%
% Based on MicromouseSymp article template by:
% Mathias Legrand (legrand.mathias@gmail.com) 
% With extensive modifications by:
% Antonio Valente (antonio.luis.valente@gmail.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[fleqn,moreauthors,10pt]{ds_report}
\usepackage[english]{babel}

\graphicspath{{fig/}}




%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

% Header
\JournalInfo{FRI Natural language processing course 2021}

% Interim or final report
\Archive{Project report} 
%\Archive{Final report} 

% Article title
\PaperTitle{Natural language processing course Latex Template} 

% Authors (student competitors) and their info
\Authors{Jakob Mrak, Enei Sluga, and Lan Vukušič}

% Advisors
\affiliation{\textit{Advisors: Slavko Žitnik}}

% Keywords
\Keywords{LLM, discourse}
\newcommand{\keywordname}{Keywords}


%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\Abstract{
This project aims to develop a highly reliable language model for qualitative discourse analysis, a critical method in social science research for understanding human interaction. The task involves categorizing postings in online discussions, such as those about the story "The Lady, or the Tiger?" using a coded dataset with high inter-rater reliability and a comprehensive codebook. The project addresses the challenge of achieving high inter-rater reliability, which is crucial for ensuring consistency and accuracy in qualitative research. This is particularly important in qualitative discourse analysis, where the nuanced understanding of human interaction requires a deep comprehension of the discussion context, each participant's perspective, and the intertextual relationships between sentences. The project leverages large language models (LLMs) to automate this labour-intensive task, aiming to generalize the model to other online discussions. This approach not only enhances the efficiency of qualitative discourse analysis but also contributes to the broader field of social science research by providing a scalable solution for analyzing complex human interactions in digital spaces.

}
%----------------------------------------------------------------------------------------

\begin{document}

% Makes all text pages the same height
\flushbottom 

% Print the title and abstract box
\maketitle 

% Removes page numbering from the first page
\thispagestyle{empty} 

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section*{Introduction}
% Report containing Introduction, existing solutions/related work and initial ideas
	
In this report, we will delve into the significance and objectives of our project, which aims to leverage large language models (LLMs) for qualitative discourse analysis, a critical method in social science research. This project addresses the challenge of achieving high inter-rater reliability in the categorization of postings in online discussions, such as those about the story "The Lady, or the Tiger?" using a coded dataset with high inter-rater
reliability and a comprehensive codebook \footnote{Codebook is a structured document or dictionary that contains mappings between symbols or codes and their corresponding linguistic features or meanings}. The relevance of our work lies in its potential to enhance the efficiency of qualitative discourse analysis, a traditionally labour-intensive process requiring significant time and resources. By addressing the limitations and variability of existing LLMs, we aim to contribute to the broader field of social science research by providing a robust solution for qualitative discourse analysis. This is crucial for advancing our understanding of human interaction in digital spaces and enhancing qualitative research's efficiency and accuracy.

\subsection{ChatGPT in education}
The work ChatGPT in education: A discourse analysis of worries and
concerns on social media~\cite{chatgpt} employs a comprehensive research framework for analyzing discourse surrounding ChatGPT in the realm of education on Twitter. Their study begins with data collection, focusing on tweets related to ChatGPT and education. They then utilize sentiment analysis, leveraging a sentiment model based on the RoBERTa architecture, to classify the sentiment of collected tweets. This sentiment analysis enables them to discern the attitudes and opinions expressed towards ChatGPT within educational contexts. Furthermore, they employ BERT-based topic modelling techniques to uncover semantic themes within the collected tweets. This approach involves utilizing BERT embedding to extract semantically relevant sentence embeddings, followed by dimensionality reduction using Uniform Manifold Approximation and Projection (UMAP) to construct coherent topic clusters. Finally, they utilize K-Means clustering and class-based Term Frequency-Inverse Document Frequency (c-TF-IDF) to represent and interpret these topics, facilitating a deeper understanding of the discourse landscape surrounding ChatGPT in educational settings.

While this work focused on sentiment analysis and topic modelling within the context of ChatGPT in education, our project aims to develop a highly reliable language model for qualitative discourse analysis across diverse online discussion topics. By leveraging large language models (LLMs) and a coded dataset with high inter-rater reliability, we seek to automate the process of categorizing postings in online discussions, thereby contributing to a scalable solution for analyzing complex human interactions in digital spaces. Thus, while \emph{ChatGPT in education}~\cite{chatgpt} provides valuable insights into the discourse surrounding ChatGPT in education, our work extends beyond sentiment analysis and topic modelling to address the broader challenge of qualitative discourse analysis in social science research.

\subsection{TopicGPT}
In the article~\cite{pham2023topicgpt} they introduce TopicGPT, a novel framework that leverages large language models (LLMs) to enhance the process of topic modelling in text corpora. Traditional topic models, such as Latent Dirichlet Allocation (LDA), often represent topics as bags of words, which can be difficult to interpret and offer limited semantic control.
TopicGPT addresses these limitations by using LLMs to uncover latent topics within a text collection, producing topics that align better with human categorizations.
Its strengths lie in generating topics based on seen textual examples. This allows this approach to refine the set of topics to a more coarse or fine set, depending on the analyzed conversation. 
Additionally, TopicGPT is adaptable, allowing users to specify constraints and modify topics without retraining the model.

This work is relevant to our project as it demonstrates the potential of LLMs in improving the efficiency and interpretability of text analysis tasks, which is a key goal of our project to develop a highly reliable language model for qualitative discourse analysis. The adaptability and semantic understanding of TopicGPT could provide valuable insights for developing a language model that not only categorizes text effectively but also offers a level of semantic understanding and interpretability that aligns with the goals of qualitative discourse analysis.


\section*{Data}

The dataset utilized in our study originates from an online discussion centered around the narrative of "The Lady, or the Tiger?" Each message within this discussion has been systematically categorized into distinct discussion types, enabling us to leverage supervised learning techniques for classification purposes. Given the relatively small size of the dataset, comprising approximately 400 messages, we will adopt few-shot learning strategies to effectively train our model. Additionally, the dataset includes supplementary annotations such as dialogue spells and pivot points, which could offer valuable insights if predicted accurately. However, these annotations present challenges due to their inconsistent, sparse nature, necessitating additional preprocessing efforts to render them usable.

\section*{Methods}

We have focused on topic prediction as our main task. The approaches we've tried are described in the following sections.
 
\subsection{Sentence embeddings}\label{sentence-embedings}
Our following approaches heavily rely on the usage of embedding vectors for further processing. We theorize, that given the size of the dataset, which is between 500 and 1000 samples, an approach using general pre-trained sentence embedings is the only viable approach. Finetuning or training a whole LLM \footnote{Large language model} would be impractical and prone to severe overfitting.

We chose a variety of pretrained sentence embedings, from the Huggingace's Sentence-transformers class of models.
\footnote{Sentance transformers is a library that allows easy use of models pretrained for sentence similarity and semantic search, available here https://huggingface.co/sentence-transformers}.
This is due to their ease of use, which allowed us to perform more tests.The models can also be trained an run locally, even on machines with no GPU, with reasonable inference speed \footnote{model is trained in less than  minute and inference is below one second for a batch of 100 sentances.}

\subsection{Clustering}
Our goal at first was to try to use sentence transformers to embed the messages from our dataset and then use clustering to group them.
This way we could have a better understanding of the data and see if there are any patterns in the data and if they correlate to the provided ground truth labels.
We chose to use the sentence transformers library because it is a very powerful tool for embedding sentences and it is very easy to use.
We selected four different sentence transformers models to embed the data and then we used the KMeans and Agglomerative clustering to cluster the data.
We used TSNE to visualize the clusters and see if there are any patterns in the data.

\subsubsection{K-Means Clustering}
K-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n-observations into k-clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster.
At first our aim was to define the number of clusters as the number of unique labels in the dataset, but we later realised that this is not the best approach because some classes are not well represented in the dataset and the model could not identify the patterns in the data well.
We found that the best number of clusters is 6, which is the number of the most represented classes in the dataset.
On \ref{fig:kmeans} we can see the clusters that the KMeans model identified for embeddings from the transformer all-mpnet-base-v2, 
which was the best performing model for the clustering task. It is visible that the model identified some patterns in the data, but the clusters are not well seperated, 
which was expected given the nature of the provided data.


\begin{figure}[htb]
    \centering
    \includegraphics[width=1\columnwidth]{fig/clustering_Kmeans_all_mpnet_base_v2.png}

    \caption{The graph shows the clusters that the KMeans model identified for transformer all-mpnet-base-v2.}
    \label{fig:kmeans}
\end{figure}

In table \ref{tab:clusters_kmeans} we look at each cluster in more detail to see which classes are represented in each cluster. At first glance we can see that in most clusters,
discourse type "Seminar" is in majority, which was expected since \(54.7\)\% of all data is of class "Seminar". In case of cluster 0, we can see that we have a majority of class "Social" and in cluster 5 
we have a majority of class "Deliberation". This shows that the model was able to identify some patterns in the data that correlate to the provided labels, but mostly the clusters do not represent the given labels well.

\begin{table}[h]
    \centering
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline
    \textbf{Clusters} & \textbf{Seminar} & \textbf{Social} & \textbf{Procedure} & \textbf{UX} & \textbf{Deliberation} & \textbf{Imaginative e.} & \textbf{Rest of} \\ \hline
    Cluster 0 & 2.94\% & \textbf{47.06\%} & 27.45\% & 12.75\% & 8.82\% & 0\% & 0.98\% \\ \hline
    Cluster 1 & \textbf{76.81\%} & 0\% & 0\% & 0\% & 5.80\% & 17.39\% & 0\% \\ \hline
    Cluster 2 &\textbf{ 94.57\%} & 0\% & 0\% & 0\% & 2.17\% & 1.09\% & 2.17\% \\ \hline
    Cluster 3 & \textbf{45.07\%} & 10.56\% & 5.63\% & 11.97\% & 19.01\% & 2.11\% & 5.65\% \\ \hline
    Cluster 4 & \textbf{96.23\%} & 0\% & 0\% & 0\% & 0.94\% & 1.89\% & 0.94\% \\ \hline
    Cluster 5 & 24.00\% & 6.00\% & 10.00\% & 17.00\% & \textbf{42.00\%} & 0\% & 1\% \\ \hline
    \end{tabular}%
    }
    \caption{Cluster composition percentages using K-means with embeddings from transformer all-mpnet-base-v2.}
    \label{tab:clusters_kmeans}
    \end{table}

\subsubsection{Agglomerative Clustering}
Agglomerative clustering is a type of hierarchical clustering that builds a hierarchy of clusters. The model starts with each point as a separate cluster and then merges the closest pairs of clusters until only one cluster remains.
We tested different linkage methods, the most promising one was 'complete' which uses the maximum distances between all observations of the two sets.
We used the same number of clusters as in the KMeans model, which is 6. We can observe the results in table \ref{tab:clusters_agglomerative},
where we can see the clusters that the Agglomerative model identified for embeddings from the transformer all-mpnet-base-v2.
In cluster 0, we can see that the model correctly clustered 90.04\% of the data as class "Seminar", which is expected, but it still shows
that the model was able to identify some patterns in the data that correlate to the provided labels. 
The difference between the KMeans and Agglomerative clustering is that the Agglomerative model was able to cluster the data somewhat better, but the clusters are still not well separated.
In the case of agglomerative clustering, the clusters exhibit distinct compositions. Notably, there are three clusters (Cluster 1, Cluster 3, and Cluster 4) where the majority of the data does not belong to the "Seminar" class, which is the most represented class in the dataset. This indicates that these clusters contain data points that are characterized by different attributes compared to the majority of the dataset.
This shows some potential for the model to identify patterns in the data that correlate to the provided labels, but ultimately the clusters do not represent the given labels well enough
to be able to predict the discourse type of the given data.


\begin{table}[h]
    \centering
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline
    \textbf{Clusters} & \textbf{Seminar} & \textbf{Social} & \textbf{Procedure} & \textbf{UX} & \textbf{Deliberation} & \textbf{Imaginative e.} & \textbf{Rest of} \\ \hline
    Cluster 0 & \textbf{90.04\%} & 0\% & 0\% & 0.99\% & 3.98\% & 3.98\% & 1.01\% \\ \hline
    Cluster 1 & 22.09\% & 4.65\% & 15.11\% & 12.79\% & \textbf{43.02\%} & 1.16\% & 0.18\% \\ \hline
    Cluster 2 & \textbf{29.54\%} & 22.73\% & 0\% & 15.90\% & 17.04\% & 0\% & 14.79\% \\ \hline
    Cluster 3 & 0\% & 0\% & 5.56\% & \textbf{61.11\%} & 33.33\% & 0\% & 0\% \\ \hline
    Cluster 4 & 4.7\% & \textbf{47.05\%} & 25.88\% & 7.05\% & 14.11\% & 0\% & 1.21\% \\ \hline
    Cluster 5 & \textbf{77.44\%} & 3.76\% & 1.5\% & 2.25\% & 5.26\% & 7.51\% & 2.28\% \\ \hline
    \end{tabular}%
    }
    \caption{Cluster composition percentages using agglomerative clustering with embeddings from transformer all-mpnet-base-v2.}
    \label{tab:clusters_agglomerative}
    \end{table}


\subsection{Classification model}
In this approach, all input sentences have been individually embedded using the previously mentioned sentence embedding techniques \ref{sentence-embedings}. This is the input for the model, that needs to predict in which of the 8 classes does the text belong to. \\

Compound classes (entries labeled, belonging to more classes) are removed, due to their low occurrence (1 or 2). Here is the table with classes and their respected counts. \\




As seen classes are unbalanced. To mitigate this, cross entropy loss is weighted by the count of samples in the predicted class, so that minority classes don't get ignored.

A shallow 2-layer 32 hidden neuron perceptron is used, to predict the classes. Larger networks were too prone to overfitting.

\begin{figure}[!htb]
    \centering
    \includegraphics[width=0.8\linewidth]{confusion_matrix.png}
    \caption{Model confusion matrix on validation data shows, that model correctly predicts the majority class but struggles with less used ones.}
    \label{fig:conf_matrix_model}
\end{figure}

Testing the model in interactive mode gave some convincing results. We tried some made-up sentences a concluded that the model predicts \textbf{Seminar} and \textbf{Social} classes with high certainty but confuses \textbf{Deliberation} and \textbf{Procedure}. \\
We believe that part of the problem is that some text samples are difficult and sometimes deceiving for a human, and we (three team members) could not unanimously decide on the correct label. \\

Overall, we consider this approach successful. In the future, better results can be achieved by providing additional context to the model if parts of the conversation are tempoarly correlated. We believe, that Domain-specific or context-aware embeddings would also greatly improve model performance. 

\subsection{TF-IDF embeddings}
Next we decided to try a classical approach by using TF-IDF embeddings. We used the TfidfVectorizer from the scikit-learn library to transform the text data into numerical data. 
First we preprocessed the text data by removing stopwords, punctuation, converting the text to lowercase and lemmatizing the text. Then we constructed the TF-IDF matrix with the TfidfVectorizer.
We then decided to try three different models to classify the data: logistic regression, support vector machine and random forest. We used the scikit-learn library to implement the models, TF-IDF matrix as input data, the provided labels as target data and 5-fold cross-validation to evaluate the models.
The results are shown in the table \ref{tab:tfidf_results}.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Model} & \textbf{Accuracy} & \textbf{Standard deviation} \\ \hline
    Logistic regression & 0.57 & 0.01 \\ \hline
    Support vector machine & 0.56 & 0.01 \\ \hline
    \textbf{Random forest} & \textbf{0.61} & \textbf{ 0.02} \\ \hline
    \end{tabular}

    \caption{Results of the models using TF-IDF embeddings.}
    \label{tab:tfidf_results}
\end{table}

As we can see, the random forest model performed the best with an accuracy of 0.61. 
Given the fact that our dataset has 6 classes that have less then 1\% of the data since they are a combination of the majority classes, we decided to either merge them into one class or remove them from the dataset.
After removing the minority classes, we tried the same models again and the results are shown in the table \ref{tab:tfidf_results_no_compound}.

\begin{table}[h]
    \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{Model} & \textbf{Accuracy} & \textbf{Standard deviation} \\ \hline
    Logistic regression & 0.59 & 0.01 \\ \hline
    Support vector machine & 0.57 & 0.01 \\ \hline
    \textbf{Random forest} & \textbf{0.64} & \textbf{0.05} \\ \hline
    \end{tabular}

    \caption{Results of the models using TF-IDF embeddings without minority classes.}
    \label{tab:tfidf_results_no_compound}
\end{table}

As we can see, the performance of the models improved after removing the minority classes. The random forest model performed the best with an accuracy of 0.64.
Given the fact that the dataset is small and the classes are unbalanced, the results are satisfactory. The majority class 'Seminar' has 54.5\% of the data (55.6\% after removal of minority classes), so the models are biased towards this class.
This means that the Random forest model has about 10\% better accuracy than a model that would predict only the majority class. Given the nature of the dataset, the results are satisfactory.


\section*{Results}










%------------------------------------------------

\section*{Discussion}

Use the Discussion section to objectively evaluate your work, do not just put praise on everything you did, be critical and exposes flaws and weaknesses of your solution. You can also explain what you would do differently if you would be able to start again and what upgrades could be done on the project in the future.


%------------------------------------------------



%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
\bibliographystyle{unsrt}
\bibliography{report}


\end{document}